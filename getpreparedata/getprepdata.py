import pandas as pd
import tpqoa
from datetime import datetime, timedelta
import logging
import os
import os.path
import pickle

import sys
sys.path.append('../')
import configs.config as cfg
from common import utils as u


class OandaDataCollector():
    '''
    Class that downloads data from Oanda for a specific instrument, can create features and lagged features.
    It can save the data into a set of folders defined in the specific instrument config module
    '''

    def __init__(self,
                 instrument,
                 labels,
                 features,
                 conf_file,
                 namefiles_dict):
        # Todo: pass here also the instrument conf file, and in that all the filenames below will be defined

        self.instrument = instrument
        self.labels = labels #"dir", "profit_over_spread", "loss_over_spread"]
        self.features = features

        # raw tick data, and resampled version which has precise time interval
        self.raw_data = None
        self.raw_data_featured = None
        self.raw_data_featured_resampled = None

        # 3 datasets as generated by the division of raw_data_resampled, but not standardized
        # these datasets contain however labelling information that should be used, due to the fact
        # that starndardization affects their value as well
        self.train_ds = None
        self.validation_ds = None
        self.test_ds = None

        # 3 datasets obtained from the 3 above via starndardization.
        # Labelling values for market direction
        # are not to be taken from here
        self.train_ds_std = None
        self.validation_ds_std = None
        self.test_ds_std = None

        # dict to host mu and std for all features, computed on training data
        self.params = {}

        # tpqoa object to download historical data
        self.api_oanda = tpqoa.tpqoa(conf_file)

        # create/get dictionary of data filenames and folders
        self.namefiles_dict = namefiles_dict.copy()

        if os.path.exists(self.namefiles_dict["base_data_folder_name"]):
            logging.info("__init__: Base folder exists: you may be overwriting existing files!")
            # Todo add choice to break out?
        else:
            logging.info("__init__: Non existent Base folder: creating it...")
            os.mkdir(self.namefiles_dict["base_data_folder_name"])
            os.mkdir(self.namefiles_dict["train_folder"])
            os.mkdir(self.namefiles_dict["valid_folder"])
            os.mkdir(self.namefiles_dict["test_folder"])
        return

    def load_data_from_file(self):
        '''load raw data and process data from file'''
        # raw data
        self.raw_data = pd.read_csv(self.namefiles_dict["raw_data_file_name"],
                                    index_col=None,header=0)
        self.raw_data_featured_resampled = pd.read_csv(
            self.namefiles_dict["raw_data_featured_resampled_file_name"],
                                    index_col=None, header=0)

        # loading 3 datasets, standardized, which contains also the columns for labels
        self.train_ds_std = pd.read_csv(self.namefiles_dict["train_filename"],
                                        index_col=None, header=0)
        self.validation_ds_std = pd.read_csv(self.namefiles_dict["valid_filename"],
                                        index_col=None, header=0)
        self.test_ds_std = pd.read_csv(self.namefiles_dict["test_filename"],
                                       index_col=None,header=0)
        self.params = pickle.load(open(self.namefiles_dict["train_folder"]  + "params.pkl", "rb"))
        return

    def report(self):
        '''provides insights on data memorized in the odc object'''
        datalist = {"raw_data" : self.raw_data,
                    "raw_data_featured_resampled": self.raw_data_featured_resampled,
                    "train_ds_std": self.train_ds_std,
                    "validation_ds_std" : self.validation_ds_std,
                    "test_ds_std" : self.test_ds_std}
        for k, df in datalist.items():
            if df is not None:
                print("\nreport: Display info for dataframe {}".format(k))
                df.info()
                print("\n")
            else:
                print("\nreport: Dataframe {} is None".format(k))
        print("\nreport: displaying information about training set statistics")
        print("report: params is {}".format(self.params))
        return

    def get_most_recent(self, granul="S30", days = 2): #S5
        '''
        Get the most recent data for the instrument for which the object was created
        :param granul: base frequency for raw data being downloaded
        :param days: number of past days (from today) we are downloading data for
        '''
        # set start and end date for historical data retrieval
        now = datetime.utcnow()
        now = now - timedelta(microseconds = now.microsecond) # Oanda does not deal with microseconds
        past = now - timedelta(days = days)
        # get historical data up to now
        logging.info("get_most_recent: calling tpqoa get_history....")
        self.raw_data = self.api_oanda.get_history(
            instrument = self.instrument,
            start = past,
            end = now,
            granularity = granul,
            price = "M",
            localize = False).c.dropna().to_frame()
        self.raw_data.rename(columns={"c": self.instrument}, inplace=True)
        print("get_most_recent: self.raw_data.info() ", self.raw_data.info())
        print("get_most_recent: self.raw_data ", self.raw_data)

        whnull = self.raw_data.isnull()
        row_has_nan = whnull.any(axis=1)
        nanrows = self.raw_data[row_has_nan]

        assert (not self.raw_data.isnull().values.any()), \
            "get_most_recent: 1-NANs in self.raw_data_featured" #
        return

    def make_features(self, window = 10, sma_int=5, hspread_ptc=0.00007):
        '''
        creates features using utils.make_features
        '''
        df = self.raw_data.copy()
        ref_price = self.instrument
        # Todo: do this check better
        assert (len(df[ref_price]) > sma_int), \
            "make_features: the dataframe lenght is not greater than the Simple Moving Average interval"
        assert (len(df[ref_price]) > window), \
            "make_features: the dataframe lenght is not greater than the Bollinger window"

        self.raw_data_featured = u.make_features(df, sma_int, window, hspread_ptc, ref_price=ref_price )
        logging.info("make_features: created new features and added to self.raw_data_featured")
        print("make_features: self.raw_data_featured.columns ", self.raw_data_featured.columns)

        assert (not self.raw_data_featured.isnull().values.any()), \
            "make_features: 1-NANs in self.raw_data_featured"
        return

    def make_lagged_features(self, lags=5):
        '''
        Add lagged features to data.
        Creates features using utils.make_lagged_features
        '''
        assert (not self.raw_data_featured.isnull().values.any()), \
            "make_lagged_features: 1-NANs in self.raw_data_featured"

        self.raw_data_featured = u.make_lagged_features(self.raw_data_featured,
                                               self.features,
                                               lags=lags)
        assert (not self.raw_data_featured.isnull().values.any()), \
            "make_lagged_features: 2-NANs in self.raw_data_featured"
        logging.info("make_lagged_features: created new features and added to self.raw_data_featured")
        return

    def resample_data(self,  brl="1min"):
        '''
        Resample data already obtained to the frequency defined by brl
        :param brl: default 1min
        '''
        print("SEQUENCE: resample_data")
        bar_length = pd.to_timedelta(brl)
        # resampling data at the desired bar length,
        # holding the last value of the bar period (.last())
        # label right is to set the index to the end of the bar time
        # dropna is here to remove weekends.
        # iloc to remove the last bar/row typically incomplete
        assert (not self.raw_data_featured.isnull().values.any()), \
            "resample_data: 1-NANs in self.raw_data_featured"

        self.raw_data_featured_resampled = \
            self.raw_data_featured.resample(bar_length,
                            label = "right").last().dropna().iloc[:-1]

        assert (not self.raw_data_featured.isnull().values.any()), \
            "resample_data: 2-NANs in self.raw_data_featured"
        assert (not self.raw_data_featured_resampled.isnull().values.any()), \
            "resample_data: 2-NANs in self.raw_data_featured_resampled"

        logging.info("resample_data: resampled the just created new features, into self.raw_data_featured_resampled")
        return

    def make_3_datasets(self, split_pcs=(0.8, 0.05, 0.15)):
        '''
        Generate 3 datasets for ML training/evaluation/test and save to files.
        '''
        if self.raw_data_featured_resampled is not None: # it was populated in precedence
            df = self.raw_data_featured_resampled
        else:
            logging.info("make_3_datasets: ERROR: self.raw_data_featured_resampled was empty!")
            #df = self.raw_data
            exit()
        assert (sum(split_pcs) == 1), "make_3_datasets: split points are not dividing the unity"
        assert (not self.raw_data_featured_resampled.isnull().values.any()), \
            "make_3_datasets: NANs in self.raw_data_featured_resampled"

        train_split = int(len(df) * split_pcs[0])
        val_split = int(len(df) *(split_pcs[0] + split_pcs[1]))
        self.train_ds = df.iloc[:train_split].copy()
        self.validation_ds = df.iloc[train_split:val_split].copy()
        self.test_ds = df.iloc[val_split:].copy()
        print("make_3_datasets: self.train_ds.info() ",self.train_ds.info())
        return

    def standardize(self):
        '''
        standardize the 3 datasets, using mean and std from train dataset
        to be called after make_3_datasets
        '''
        logging.info("standardize: subtracting mean and dividing by standard deviation, for each feature!")
        mu, std = self.train_ds.mean(), self.train_ds.std()
        print("standardize: std: ", std)
        # standardize all of them using training data derived statistics
        self.train_ds_std = (self.train_ds - mu) / std
        self.validation_ds_std = (self.validation_ds - mu) / std
        self.test_ds_std = (self.test_ds - mu) / std
        self.params = {"mu": mu, "std": std}
        print("standardize: self.train_ds_std.info() ", self.train_ds_std.info())
        assert (not self.train_ds_std.isnull().values.any()), "standardize: NANs in Training Data"
        return

    def save_to_file(self):
        '''Save the previously formed datasets to disk'''
        logging.info("save_to_file: Saving raw data and resampled raw data to {} and to {}".format(
                self.namefiles_dict["raw_data_file_name"],
                  self.namefiles_dict["raw_data_featured_resampled_file_name"]))

        #TODO: should save index=True for self.namefiles_dict["raw_data_file_name"] so as to be free to resample at different
        # frequency later on, as if I want to add a new feature, I start from raw data, add the feature
        # make the lagged versions of all of them and then resample to get the frequency
        # so in order to get the new feature, I need resampling to work on the data I saved previously
        # therefore I need to save here the index, so resampling has the reference values...
        self.raw_data.to_csv(self.namefiles_dict["raw_data_file_name"],
                             index = False, header=True)
        self.raw_data_featured_resampled.to_csv(self.namefiles_dict["raw_data_featured_resampled_file_name"],
                                                index = False, header=True)

        logging.info('save_to_file: Saving data input files to {}'.format(self.namefiles_dict["base_data_folder_name"]))
        self.train_ds_std.to_csv(self.namefiles_dict["train_filename"], index = False, header=True)
        logging.info("save_to_file: Save train_ds_std to {}".format(self.namefiles_dict["train_filename"]))
        self.validation_ds_std.to_csv(self.namefiles_dict["valid_filename"], index = False, header=True)
        logging.info("save_to_file: Save validation_ds_std to {}".format(self.namefiles_dict["valid_filename"]))
        self.test_ds_std.to_csv(self.namefiles_dict["test_filename"], index = False, header=True)
        logging.info("save_to_file: Save test_ds_std to {}".format(self.namefiles_dict["test_filename"]))

        logging.info('save_to_file: saving data label files to {}'.format(self.namefiles_dict["base_data_folder_name"]))
        self.train_ds[self.labels].to_csv(self.namefiles_dict["train_labl_filename"], index = False, header=True)
        self.validation_ds[self.labels].to_csv(self.namefiles_dict["valid_labl_filename"], index = False, header=True)
        self.test_ds[self.labels].to_csv(self.namefiles_dict["test_labl_filename"], index = False, header=True)

        logging.info('save_to_file: saving params to file {}'.format(self.namefiles_dict["train_folder"]  + "params.pkl"))
        pickle.dump(self.params, open(self.namefiles_dict["train_folder"]  + "params.pkl", "wb"))
        return


if __name__ == '__main__':

    '''
    __main__ execute functional test, or it can be used to download data for later use
    '''
    # change this import pointing to the wanted/needed configuration for the main to work
    import configs.EUR_PLN_1 as cfginst

    # get or generate datafiles files and folders, if do not exist
    namefiles_dict = {}
    namefiles_dict = u.creates_filenames_dict(cfginst.instrument, namefiles_dict, cfg)

    odc = OandaDataCollector(instrument=cfginst.instrument,
                             labels=cfginst.labels,
                             features=cfginst.features,
                             conf_file=cfg.conf_file,
                             namefiles_dict=namefiles_dict)
    print('OandaDataCollector object created for instrument {}'.format(cfginst.instrument))
    NEW_DATA = True
    if NEW_DATA:
        # actual data collection of most recent data
        print('OandaDataCollector data collection starts...')
        odc.get_most_recent(granul=cfginst.granul, days = cfginst.days)
        odc.make_features()
        odc.make_lagged_features()
        odc.resample_data(brl = cfginst.brl)
        odc.make_3_datasets()
        odc.standardize()
        odc.save_to_file()

        print("All row data downloaded from Oanda for instrument {}".format(cfginst.instrument))
        print(odc.raw_data.info(),  end="\n  ******** \n")
        print("Re-sampled data for bar length {} from Oanda for instrument {}".format(
                                                                cfginst.brl, cfginst.instrument))
        print(odc.raw_data_featured_resampled.info(),  end="\n  ******** \n")
    else:
        print('OandaDataCollector data is loading from disk...')
        odc.load_data_from_file()
        odc.report()
        # odc.make_features()
        # odc.make_lagged_features()
        # odc.report()





