03.05.2021

DONE:
- complete the chain adding the trading part. Start simple with loading the pretrained model, streaming live
data and setting orders based on the model output(s). See trader.py? o meglio DNNtrader.py

- prepare real backtesting of the DNNTrader class, which indeed includes a strategy that we
want to backtest with graphical visualization of the model behaviour on a long period

- add trading costs (estimated with fixed half price)

- try with other instruments

- use data representation that keeps time reference so to have time perspective in charts and evaluation

- move the project to juniper notebooks to visualize, still remaining as pycharm project
as jpnb: trader_ml.py, trainer.py, getpreparedata.py

Todo (Backlog):

- make trainer.py a parent class: then inherit and specialize for each model in models

- data format used for Data folder is not perfect for time-aware approaches (RNN/Transf): review!

- review dimensionality and balance of feature values

-  add confusion matrix generation in trainer (tf2.0 course ann mnist)

- get data to calculate the spread, and with the assumption that spread does not change much
 include spread in strategy, or simply make sure trading happens in busy hours when spread is low

- Why is bollinger often ending up in non-computed/nan???? see in getpreparedata and common/utils.py
  Because dividing by std computed in a window is risky, for some time data is constant for missing data
   and therefore std goes to 0. Review how to work with it, for now replaced / with *...

- instrument configuration: it must be an input file and not a python module

- TESTs

- review why ticks are so async... what is the origin of the asynchronicity?

- add more tasks to DNN, add loss function components and optimize the overall loss

- data: make getpreparedata able to work on raw data saved on disk.
it should be possible to add new features, resample, generate new train,eval,test datasets
in order to do that, I must save the index timedata to csv and load it correclty at a later time
so I can resample it taking into consideration the timeindex


- implement architecture based on autoencoding a 2d input, initially fixed to white sheet (all 0).
The autoencoder concatenates a representation of the input at time t_(-n) and generate an output
This output is then added to the original input via skip connection and layer normalized (so not to diverge)
This is repeated as many time as the input sequence in long. The final 2D tensor is fed into a conv net to
classify it for many tasks (market direction, increase of close price beyond a threshold, or a a decrease
below a threshold.

         .........................................

        |        |||  ===>concat<===     ||
        |       |||||                   |||||
        |    ||||||||||||             |||||||||
        |_____ L1 2D               t_(-n+1) inputs
|-----------  +O+
|                |
|            ||||||||
|             ||||||
|       || ===>concat<===||
|     |||||            |||||
|  ||||||||||||      |||||||||
|    ___|
L0=0 2D             t_(-n) inputs