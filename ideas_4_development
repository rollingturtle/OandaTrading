03.05.2021

DONE:
- complete the chain adding the trading part. Start simple with loading the pretrained model, streaming live
data and setting orders based on the model output(s). See trader.py? o meglio DNNtrader.py
- prepare real backtesting of the DNNTrader class, which indeed includes a strategy that we
want to backtest with graphical visualization of the model behaviour on a long period
- add trading costs (estimated with fixed half price)
- try with other instruments
- use data representation that keeps time reference so to have time perspective in charts and evaluation
- move the project to juniper notebooks to visualize, still remaining as pycharm project
as jpnb: trader_ml.py, trainer.py, getpreparedata.py
- data format used for Data folder is not perfect for time-aware approaches (RNN/Transf): review!
- review feature definitions - still bollinger is ill defined in the feaures being used!
- Why is bollinger often ending up in non-computed/nan???? see in getpreparedata and common/utils.py
  Because dividing by std computed in a window is risky, for some time data is constant for missing data
   and therefore std goes to 0. Review how to work with it, for now replaced / with *...
- try go multithreaded for data collection
- see how to get the instant spread value from Oanda (2 threads with 2 stream?)
- review the target for the model: what I may want to get is whether or not the average of the
log returns in the next, say, 6 steps (t+0=NOW -> t+5), is positive or negative.
Even better, if it is greater than the spread at time t
Averaging log returns should yield an entity that is smoother and possibly easier to predict.
- review dimensionality and balance of feature values, review features
- Add more features on make_features utility, not only from close price. Now get_most_recent obtains
info for bid and ask prices, and also volume can be informative
- make possible to build new feature or re-built 3 datasets from raw data without always downloading
from Oanda. Risk to be banned...
- collect epochs and dropout params in better place
- data: make getpreparedata able to work on raw data saved on disk.
it should be possible to add new features, resample, generate new train,eval,test datasets
in order to do that, I must save the index timedata to csv and load it correclty at a later time
so I can resample it taking into consideration the timeindex


Todo (Backlog):
- get ready to move on colab or on gpu intel
-trainer: add callback to save model at best validation accuracy

- get bid and ask for historical and do better backtesting/fwtesting with better trading cost estimation
maybe with the assumption that spread does not change much
In general, include spread in strategy, or simply make sure trading happens in busy hours when spread is low


- consider Time Conv Net

- First step towards control_class: all with 1 instrument, but pull everything under control_class

- consider control_class (compositional approach): one super class that instantiates:
   OandaDataCollector
   DL_Trainer
   DNNTrader
   The control_class should take a list of instruments (or one only) for the data collection
    but also a list of instruments to trade.
    Here the open point of how to dowload live data for multiple instruments
    v20 supports it but tpqoa apparently not. Either I do my multithreading or I call v20 directly


- consider training on a joint data from a "connected" group of instruments, so to increase
the correlation that a single instrument has with all the others.
e.g. EUR_PLN, USD_PLN, EUR_USD and train to make predictions on each of three using all the data

- save models with callback based on eval accuracy

- make trainer.py a parent class: then inherit and specialize for each model in models

- strategies: consider exponential decay toward more permissive threshold for a taken position that is
wrong.... and zero out the counter of decay any time the position is changed...

- under trained models I should differentiate by conf file and version of it

-  add confusion matrix generation in trainer (tf2.0 course ann mnist)

- instrument configuration: it must be an input file and not a python module

- TESTs: add test folder and write them with pytest

- review why ticks are so async... what is the origin of the asynchronicity?

- add more tasks to DNN, add loss function components and optimize the overall loss

- implement architecture based on autoencoding a 2d input, initially fixed to white sheet (all 0).
The autoencoder concatenates a representation of the input at time t_(-n) and generate an output
This output is then added to the original input via skip connection and layer normalized (so not to diverge)
This is repeated as many time as the input sequence in long. The final 2D tensor is fed into a conv net to
classify it for many tasks (market direction, increase of close price beyond a threshold, or a a decrease
below a threshold.

         .........................................

        |        |||  ===>concat<===     ||
        |       |||||                   |||||
        |    ||||||||||||             |||||||||
        |_____ L1 2D               t_(-n+1) inputs
|-----------  +O+
|                |
|            ||||||||
|             ||||||
|       || ===>concat<===||
|     |||||            |||||
|  ||||||||||||      |||||||||
|    ___|
L0=0 2D             t_(-n) inputs